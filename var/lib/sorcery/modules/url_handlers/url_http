#!/bin/bash
#---------------------------------------------------------------------
##
##=head1 SYNOPSIS
##
## Url handler functions for downloading http, https, and ftp urls
##
##=head1 DESCRIPTION
##
## This file contains functions for downloading and verifying
## http, https, and ftp urls.  This file uses the "wget" program.
##
##=head1 COPYRIGHT
##
## Copyright 2002 by the Source Mage Team
##
##=head1 FUNCTIONS
##
##=over 4
##
#---------------------------------------------------------------------

#---------------------------------------------------------------------
##=item url_http_bucketize <url>
##
## echoes the download handler - wget
##
#---------------------------------------------------------------------
url_http_bucketize() {
  echo $HTTP_DL_HANDLER
}
url_ftp_bucketize() {
  echo $HTTP_DL_HANDLER
}
url_https_bucketize() {
  echo $HTTP_DL_HANDLER
}

#---------------------------------------------------------------------
##=item url_http_verify <url>
##
## Verifies the specified http url.  Returns true if the url exists
## OR if the url is an empty string.
##
#---------------------------------------------------------------------
url_http_verify() {
  local  URL=$1
  local hints="$2"
  local check_certificate

  if list_find "$hints" no-check-certificate; then
    check_certificate=--no-check-certificate
  fi

  if [  -n  "$URL"  ]; then
    if  OUTPUT=`wget --passive-ftp -t 1 -T 30 --spider $check_certificate "$URL" 2>&1`;  then
      true
    else
      echo  $OUTPUT
      false
    fi
  fi
}


#---------------------------------------------------------------------
##=item url_https_verify <url>
##
## Verifies the specified https url.  Returns true if the url exists
## OR if the url is an empty string.
##
#---------------------------------------------------------------------
url_https_verify() {
  url_http_verify  "$@"
}


#---------------------------------------------------------------------
##=item url_ftp_verify <url>
##
## Verifies the specified ftp url.  Echos results of wget if file
## is not found.
##
##
## @Implementation_note wget --spider still downloads ftp files in full rather
## @Implementation_note than just checking that the file is there. To get
## @Implementation_note around this problem, we download the directory and see
## @Implementation_note if the file is in the directory listing.
##
#---------------------------------------------------------------------
url_ftp_verify() {
  local  URL=$1

  if  [  -n  "$URL"  ];  then
    local  FILENAME
    local  DIRECTORY
    smgl_basename $URL FILENAME
    smgl_dirname $URL DIRECTORY
    local  OUTPUT=`wget --passive-ftp -t 1 -T 30 -O - --spider -S "$DIRECTORY/" 2>&1`

    if real_list_find "$OUTPUT" "$FILENAME";  then
      rm  -f  .listing
    else
      echo  $OUTPUT  |  sed  's/LIST.*//g'
      [  -f  .listing  ]  &&  cat  .listing
      rm -f  .listing
      false
    fi
  fi
}

#---------------------------------------------------------------------
##=item url_<prefix>_hostname <url>
##
## Gets the hostname out of the url
#---------------------------------------------------------------------
url_http_hostname() {
  echo $1|sed 's:^.*//\([^/]*\).*$:\1:'
}
url_https_hostname() {
  url_http_hostname $1
}
url_ftp_hostname() {
  url_http_hostname $1
}

#---------------------------------------------------------------------
##=item url_<prefix>_netselect <url>
##
## Run netselect on the url, netselect understands these urls and
## we can take advantage of its multi-A record handling
##
#---------------------------------------------------------------------
url_http_netselect() {
  netselect -t 3 -s 1000 "$@" 2>/dev/null
}
url_https_netselect() {
  netselect -t 3 -s 1000 "$@" 2>/dev/null
}
url_ftp_netselect() {
  netselect -t 3 -s 1000 "$@" 2>/dev/null
}

#-------------------------------------------------------------------------
## expand a url to ALL of the mirrors we think may be related
## Take the url, find its hostname, if a file in $MIRRORS matches
## generate new urls from all the mirrors in the file, the original url
## is kept at the top of the list and not duplicated
##
## @param a list of urls
## @stdout the expanded form of those urls
#-------------------------------------------------------------------------
url_http_expand() {
  local A=$'\a'
  local URL rep tgt my_hostname each
  # put the requested url first
  echo "$@"
  for URL in $@ ; do
    my_hostname=$(url_hostname ${URL})
    # a mirror is listed in the mirror listings as either
    # ftp://foo.somemirror.org/stuff
    # OR
    # ftp://foo.somemirror.org
    # note neither ends in a '/' but one has a '/' at the end of the
    # hostname, the other has an end of line. The \(/\|\$\) stuff matches
    # either
    for each in $(grep -lr "://$my_hostname\(/\|\$\)" --exclude-dir=config $MIRRORS); do
      rep=$(grep "://$my_hostname\(/\|\$\)" $each|awk '{print $NF; exit 0; }')
      for tgt in $(awk '{print $NF}' $each|grep -v '^Custom$'); do
         echo ${URL}|sed "s$A${rep}$A${tgt}$A"
      done
    done |grep -v $my_hostname
  done
}

url_ftp_expand() {
  url_http_expand "$@"
}
# note that there is no https_expand

#---------------------------------------------------------------------
##=back
##
##=head1 LICENSE
##
## This software is free software; you can redistribute it and/or modify
## it under the terms of the GNU General Public License as published by
## the Free Software Foundation; either version 2 of the License, or
## (at your option) any later version.
##
## This software is distributed in the hope that it will be useful,
## but WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
## GNU General Public License for more details.
##
## You should have received a copy of the GNU General Public License
## along with this software; if not, write to the Free Software
## Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
##
#---------------------------------------------------------------------
